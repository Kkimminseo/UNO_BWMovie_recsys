import os
import pickle
import pandas as pd
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv

load_dotenv()


def process_and_store_embeddings(csv_file):
    """
    CSV íŒŒì¼ì„ ì½ì–´ì™€ ì „ì²˜ë¦¬ í›„, ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€í•˜ì—¬ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    ê¸°ì¡´ `faiss_index.faiss`, `documents.pkl`ì„ ìœ ì§€í•˜ë©´ì„œ ì—…ë°ì´íŠ¸í•¨.
    """
    # ê²½ë¡œ ìƒìˆ˜ ì •ì˜
    FAISS_DIR = "faiss"
    DOCUMENTS_PATH = os.path.join(FAISS_DIR, "documents.pkl")

    # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
    os.makedirs(FAISS_DIR, exist_ok=True)

    # 1ï¸âƒ£ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    print("ğŸ“¥ ë°ì´í„° ë¡œë“œ ì¤‘...")
    df = pd.read_csv(csv_file)

    # 'softcore' í‚¤ì›Œë“œ ì œê±°
    df = df[~df["keywords"].str.contains("softcore", case=False, na=False)]

    # í•„ìš”í•œ ì»¬ëŸ¼ ê²°í•©
    df["combined_text"] = (
        df["title"] + " " + df["overview"] + " " + df["genres"] + " " + df["keywords"]
    )
    df["combined_text"] = df["combined_text"].astype(str)

    # LangChain Document ê°ì²´ ë³€í™˜
    new_docs = [Document(page_content=text) for text in df["combined_text"].tolist()]

    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ! ì¶”ê°€í•  ë¬¸ì„œ ìˆ˜: {len(new_docs)}")

    # 2ï¸âƒ£ ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    embedding_model = OpenAIEmbeddings(model="text-embedding-ada-002")

    if os.path.exists(FAISS_DIR):
        print("ğŸ“‚ ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...")
        try:
            faiss_index = FAISS.load_local(
                FAISS_DIR, embedding_model, allow_dangerous_deserialization=True
            )
        except EOFError:
            print("âŒ ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ê°€ ì†ìƒë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
            faiss_index = FAISS.from_documents([], embedding_model)
    else:
        print("ğŸ†• ê¸°ì¡´ ì¸ë±ìŠ¤ ì—†ìŒ. ìƒˆë¡œìš´ FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        faiss_index = FAISS.from_documents([], embedding_model)

    # ê¸°ì¡´ ë¬¸ì„œ ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸°
    if os.path.exists(DOCUMENTS_PATH):
        with open(DOCUMENTS_PATH, "rb") as f:
            existing_docs = pickle.load(f)
        print(f"ğŸ“œ ê¸°ì¡´ ë¬¸ì„œ ìˆ˜: {len(existing_docs)}")
    else:
        existing_docs = []
        print("âš  ê¸°ì¡´ documents.pkl ì—†ìŒ. ìƒˆë¡œìš´ íŒŒì¼ ìƒì„±.")

    # 3ï¸âƒ£ FAISS ì¸ë±ìŠ¤ì— ìƒˆë¡œìš´ ë°ì´í„° ì¶”ê°€
    print("ğŸš€ ìƒˆë¡œìš´ ë¬¸ì„œ FAISSì— ì¶”ê°€ ì¤‘...")
    try:
        faiss_index.add_documents(new_docs)
        print(f"âœ… ì´ {len(new_docs)}ê°œì˜ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ!")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # ê¸°ì¡´ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ì— ìƒˆë¡œìš´ ë¬¸ì„œ ì¶”ê°€
    all_docs = existing_docs + new_docs

    # 4ï¸âƒ£ FAISS ë° ë©”íƒ€ë°ì´í„° ì €ì¥
    faiss_index.save_local(
        FAISS_DIR
    )  # `faiss_index.faiss`, `faiss_index.pkl` ìë™ ìƒì„±
    print("âœ… FAISS ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ!")

    with open(DOCUMENTS_PATH, "wb") as f:
        pickle.dump(all_docs, f)
    print("âœ… documents.pkl ì €ì¥ ì™„ë£Œ!")

    print("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")


# ì‹¤í–‰ ì˜ˆì œ
csv_file_path = "recent_movies.csv"  # ìƒˆë¡œìš´ CSVc íŒŒì¼ ê²½ë¡œ
process_and_store_embeddings(csv_file_path)
